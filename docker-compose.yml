version: '2.3'
volumes:
  bundle_cache: {}
  # scrapy_shared:
  db_data:
  db_backup:
  api_data:
  # bot_data:
  # search_data:
  scrapydo_data:
  splash_data:
  # mongo_data:
services:
  app:
    build:
      context: ./web
      dockerfile: ./Dockerfile
    depends_on:
      - db
      - api
      # - search
      - scrapy
    volumes:
      - bundle_cache:/bundle
      - ./scrapy/data:/var/www/scrapy/data
      - ./web/app:/var/www/alegreme/app
      - ./web/config:/var/www/alegreme/config
      - ./web/lib:/var/www/alegreme/lib
      - ./web/public/uploads:/var/www/alegreme/public/uploads
      - ./web/db/backups:/var/www/alegreme/db/backups
      - ./logs/app:/var/www/alegreme/log
      - ./storage:/var/www/alegreme/storage
    environment:
      RAILS_ENV: production
      RACK_ENV: production
      RAILS_SERVE_STATIC_FILES: 'false'
      PGHOST: db
      PGUSER: jon
      API_URL: http://api
      # ELASTICSEARCH_URL: http://search:9200
      IS_DOCKER: 'true'
      HTTP_HOST: 'www.alegreme.com'
      NODE_ENV: production
    links:
      - api
      - db
      # - search
    restart: always
    ports:
      - 3000:3000
    command: cron -f && bundle exec puma -C config/puma.rb
    mem_limit: 6048000000
  db:
    image: postgres:10
    volumes:
      - ./data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: jon
      POSTGRES_PASSWORD: password
      POSTGRES_DB: production
      TZ: 'America/Sao_Paulo'
      PGTZ: 'America/Sao_Paulo'
      PRIVATE_IP: '172.26.7.70'
      PUBLIC_IP: '3.223.33.161'
    ports:
      - 5432:5432
    restart: always
    mem_limit: 2048000000
  web:
    build:
      context: ./web
      dockerfile: ./docker/web/Dockerfile
    depends_on:
      - app
    ports:
      - 80:80
      - 443:443
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
      - ./logs/web:/var/www/alegreme/log
    restart: always
    mem_limit: 1048000000
  certbot:
    image: certbot/certbot
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
  api:
    build:
      context: ./ml
      dockerfile: ./Dockerfile
    ports:
      - "5000:5000"
    volumes:
      - api_data:/var/www/api/data
      - ./scrapy/data:/var/www/scrapy/data
      - ./logs/api:/var/www/ml/api/log
    environment:
      IS_DOCKER: 'true'
    mem_limit: 3048000000
  # search:
  #   build:
  #     context: ./search
  #     dockerfile: ./Dockerfile
  #   environment:
  #     discovery.type: single-node
  #   ports:
  #     - "9200:9200"
  #   volumes:
  #     - search_data:/var/www/search/data
  #   restart: always
  scrapy:
    build:
      context: ./scrapy
      dockerfile: ./Dockerfile
    environment:
      SPLASH_URL: http://splash:8050
      IS_DOCKER: 'true'
      PRIVATE_IP: '172.26.7.70'
      PUBLIC_IP: '3.223.33.161'
    depends_on:
      - splash
    volumes:
      - ./scrapy/data:/var/www/scrapy/data
      - scrapydo_data:/var/www/scrapy/projects
    ports:
      - "7654:7654"
    restart: always
    command: scrapy-do --nodaemon --pidfile= scrapy-do --config scrapydo.conf
    mem_limit: 3048000000
  splash:
    image: scrapinghub/splash
    ports:
      - "8050:8050"
      - "5023:5023"
    restart: always
    volumes:
      - splash_data:/var/www/splash/data
    mem_limit: 4048000000
    command: --maxrss 4048 --max-timeout 300 --disable-lua-sandbox
