version: '2.3'
volumes:
  bundle_cache: {}
  scrapy_data:
  db_data:
  db_backup:
  api_data:
  redis_data:
  splash_data:
services:
  app:
    build:
      context: ./web
      dockerfile: ./Dockerfile
    depends_on:
      - db
      - api
      - scrapy
      - redis
      - search
    volumes:
      - bundle_cache:/bundle
      - ./scrapy/data:/var/www/scrapy/data
      - ./web/public/uploads:/var/www/alegreme/public/uploads
      - ./web/db/backups:/var/www/alegreme/db/backups
      - ./logs/app:/var/www/alegreme/log
      - ./storage:/var/www/alegreme/storage
    environment:
      RAILS_ENV: production
      RACK_ENV: production
      RAILS_SERVE_STATIC_FILES: 'false'
      PGHOST: db
      PGUSER: jon
      API_URL: http://api
      IS_DOCKER: 'true'
      HTTP_HOST: 'www.alegreme.com'
      NODE_ENV: production
      WEB_CONCURRENCY: 2
      RAILS_MAX_THREADS: 4
      JOB_WORKER_URL: redis://redis:6379/0
      REDIS_URL: redis://:password@redis:6379/1
      ELASTICSEARCH_URL: http://search:9200
    restart: always
    ports:
      - 3000:3000
    command: bundle exec puma -C config/puma.rb
    mem_limit: 5gb
    memswap_limit: 6gb
    mem_reservation: 256m
    cpus: 3.5
  sidekiq:
    build:
      context: ./web
      dockerfile: ./docker/sidekiq/Dockerfile
    depends_on:
      - redis
      - db
    volumes:
      - ./web:/var/www/alegreme
      - ./logs/sidekiq:/var/www/alegreme/log
    working_dir: /var/www/alegreme
    environment:
      RAILS_ENV: production
      RACK_ENV: production
      PGHOST: db
      PGUSER: jon
      API_URL: http://api
      IS_DOCKER: 'true'
      HTTP_HOST: 'www.alegreme.com'
      NODE_ENV: production
      WEB_CONCURRENCY: 2
      RAILS_MAX_THREADS: 4
      JOB_WORKER_URL: redis://redis:6379/0
    command: bundle exec sidekiq -C /var/www/sidekiq/config.yml -r /var/www/alegreme
    restart: always
    mem_limit: 3gb
    memswap_limit: 3gb
    cpus: 2
  db:
    image: postgres:10
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: jon
      POSTGRES_PASSWORD: password
      POSTGRES_DB: production
      TZ: 'America/Sao_Paulo'
      PGTZ: 'America/Sao_Paulo'
      PRIVATE_IP: '10.136.227.110'
      PUBLIC_IP: '159.89.84.18'
    ports:
      - 5432:5432
    restart: always
    mem_limit: 2gb
    memswap_limit: 3gb
    cpus: 2
  web:
    build:
      context: ./web
      dockerfile: ./docker/web/Dockerfile
    depends_on:
      - app
    ports:
      - 80:80
      - 443:443
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
      - ./logs/web:/var/www/alegreme/log
    restart: always
    mem_limit: 2gb
    memswap_limit: 3gb
    cpus: 2
  certbot:
    image: certbot/certbot
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
  api:
    build:
      context: ./ml
      dockerfile: ./Dockerfile
    ports:
      - "5000:5000"
    volumes:
      - api_data:/var/www/api/data
      - ./scrapy/data:/var/www/scrapy/data
      - ./logs/api:/var/www/ml/api/log
    environment:
      IS_DOCKER: 'true'
    mem_limit: 4gb
    memswap_limit: 4gb
    mem_reservation: 256m
    cpus: 3
  scrapy:
    build:
      context: ./scrapy
      dockerfile: ./Dockerfile
    environment:
      SPLASH_URL: http://splash:8050
      IS_DOCKER: 'true'
      ENV: 'production'
      PRIVATE_IP: '10.136.227.110'
      PUBLIC_IP: '159.89.84.18'
    depends_on:
      - splash
    volumes:
      - ./scrapy/alegreme:/var/www/scrapy/alegreme
      - ./scrapy/data:/var/www/scrapy/data
      - ./logs/scrapy:/var/www/scrapy/log
    ports:
      - "7654:7654"
    restart: on-failure
    # command: scrapy-do --nodaemon --pidfile= scrapy-do --config scrapydo.conf
    mem_limit: 3gb
    memswap_limit: 4gb
    cpus: 2
  splash:
    image: scrapinghub/splash
    ports:
      - 8050:8050
      - 5023:5023
    restart: on-failure
    volumes:
      - splash_data:/var/www/splash/data
    mem_limit: 4gb
    memswap_limit: 5gb
    mem_reservation: 256m
    cpus: 3
    command: --maxrss 3500 --max-timeout 3600
  redis:
    image: redis
    environment:
      REDIS_PASSWORD: password
    command: redis-server --appendonly yes --requirepass password
    volumes:
      - redis_data:/data
    ports:
      - 6379:6379
    mem_limit: 3gb
    memswap_limit: 3gb
    mem_reservation: 256m
    cpus: 2
    restart: always
  search:
    build:
      context: ./search
      dockerfile: ./Dockerfile
    environment:
      discovery.type: single-node
    ports:
      - "9200:9200"
    mem_limit: 2gb
    memswap_limit: 2gb
    mem_reservation: 256m
    cpus: 1.5
    restart: always
