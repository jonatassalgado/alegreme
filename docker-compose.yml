version: '3.2'
volumes:
  bundle_cache: {}
  # scrapy_shared:
  db_data:
  db_backup:
  api_data:
  # bot_data:
  search_data:
  scrapydo_data:
  splash_data:
  # mongo_data:
services:
  app:
    build:
      context: ./web
      dockerfile: ./Dockerfile
    depends_on:
      - db
      - api
      - search
      - scrapy
    volumes:
      - bundle_cache:/bundle
      - ./scrapy/data:/var/www/scrapy/data
      - ./web/app:/var/www/alegreme/app
      - ./web/config:/var/www/alegreme/config
      - ./web/lib:/var/www/alegreme/lib
      - ./web/public/uploads:/var/www/alegreme/public/uploads
      - ./web/db/backups:/var/www/alegreme/db/backups
      - ./storage:/var/www/alegreme/storage
    environment:
      RAILS_ENV: production
      RACK_ENV: production
      RAILS_SERVE_STATIC_FILES: 'false'
      PGHOST: db
      PGUSER: jon
      API_URL: http://api
      ELASTICSEARCH_URL: http://search:9200
      IS_DOCKER: 'true'
      HTTP_HOST: 'www.alegreme.com'
      NODE_ENV: production
    links:
      - api
      - db
      - search
    restart: always
    ports:
      - 3000:3000
  db:
    image: postgres:10
    volumes:
      - ./data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: jon
      POSTGRES_PASSWORD: password
      POSTGRES_DB: production
      TZ: 'America/Sao_Paulo'
      PGTZ: 'America/Sao_Paulo'
      PRIVATE_IP: '172.26.7.70'
      PUBLIC_IP: '3.223.33.161'
    ports:
      - 5432:5432
    restart: always
  web:
    build:
      context: ./web
      dockerfile: ./docker/web/Dockerfile
    depends_on:
      - app
    ports:
      - 80:80
      - 443:443
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
    restart: always
  certbot:
    image: certbot/certbot
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
  api:
    build:
      context: ./ml
      dockerfile: ./Dockerfile
    ports:
      - "5000:5000"
    volumes:
      - api_data:/var/www/api/data
      - ./scrapy/data:/var/www/scrapy/data
    environment:
      IS_DOCKER: 'true'
  search:
    build:
      context: ./search
      dockerfile: ./Dockerfile
    environment:
      discovery.type: single-node
    ports:
      - "9200:9200"
    volumes:
      - search_data:/var/www/search/data
    restart: always
  scrapy:
    build:
      context: ./scrapy
      dockerfile: ./Dockerfile
    environment:
      SPLASH_URL: http://splash:8050
      IS_DOCKER: 'true'
      PRIVATE_IP: '172.26.7.70'
      PUBLIC_IP: '3.223.33.161'
    depends_on:
      - splash
    volumes:
      - ./scrapy/data:/var/www/scrapy/data
      - scrapydo_data:/var/www/scrapy/projects
    ports:
      - "7654:7654"
    restart: always
  splash:
    image: scrapinghub/splash
    ports:
      - "8050:8050"
      - "5023:5023"
    restart: always
    volumes:
      - splash_data:/var/www/splash/data
