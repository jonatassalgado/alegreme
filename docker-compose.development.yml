version: '2.3'
volumes:
  bundle_cache: {}
  # scrapy_shared:
  db_data:
  db_backup:
  api_data:
  # search_data:
  scrapydo_data:
  splash_data:
services:
  app:
    build:
      context: ./web
      dockerfile: ./Dockerfile
    depends_on:
      - db
      - api
      # - search
      # - bot
      - scrapy
    volumes:
      - bundle_cache:/bundle
      - ./scrapy/data:/var/www/scrapy/data
      - ./web:/var/www/alegreme
      # - ./web/app:/var/www/alegreme/app
      # - ./web/config:/var/www/alegreme/config
      # - ./web/lib:/var/www/alegreme/lib
      # - ./web/public/uploads:/var/www/alegreme/public/uploads
      # - ./web/db/backups:/var/www/alegreme/db/backups
      # - ./web/test:/var/www/alegreme/test
      - ./storage:/var/www/alegreme/storage
      - ./logs/app:/var/www/alegreme/log
      # - ./web/scrapy:/var/www/scrapy/data/scraped
    environment:
      RAILS_ENV: development
      RACK_ENV: development
      RAILS_SERVE_STATIC_FILES: 'false'
      PGHOST: db
      PGUSER: postgres
      # AWS_ACCESS_KEY_ID: REMOVED
      # AWS_SECRET_ACCESS_KEY: REMOVED
      # AWS_BUCKET: alegreme-event-cover
      API_URL: http://api
      # ELASTICSEARCH_URL: http://search:9200
      IS_DOCKER: 'true'
      HTTP_HOST: 'www.alegreme.com'
      NODE_ENV: development
    links:
      # - api
      - db
      # - search
    restart: always
    ports:
      - 3000:3000
    stdin_open: true
    tty: true
    command: bundle exec puma -C config/puma/puma-development.rb
    mem_limit: 6048000000
  db:
    image: postgres
    volumes:
      - db_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: docker
      POSTGRES_DB: db
      TZ: 'America/Sao_Paulo'
      PGTZ: 'America/Sao_Paulo'
    ports:
      - 5432:5432
    restart: always
    mem_limit: 2048000000
  # web:
  #   build:
  #     context: ./web
  #     dockerfile: ./docker/web/Dockerfile
  #   depends_on:
  #     - app
  #   ports:
  #     - 80:80
  #     - 443:443
  #   volumes:
  #     - ./data/certbot/conf:/etc/letsencrypt
  #     - ./data/certbot/www:/var/www/certbot
  #   restart: always
  # certbot:
  #   image: certbot/certbot
  #   volumes:
  #     - ./data/certbot/conf:/etc/letsencrypt
  #     - ./data/certbot/www:/var/www/certbot
  api:
    build:
      context: ./ml
      dockerfile: ./Dockerfile
    ports:
      - "5000:5000"
    volumes:
      - api_data:/var/www/api/data
      - ./scrapy/data:/var/www/scrapy/data
      - ./ml/api:/var/www/ml/api
      - ./logs/api:/var/www/ml/api/log
    environment:
      IS_DOCKER: 'true'
    mem_limit: 2048000000
  # search:
  #   build:
  #     context: ./search
  #     dockerfile: ./Dockerfile
  #   environment:
  #     discovery.type: single-node
  #   ports:
  #     - "9200:9200"
  #   volumes:
  #     - search_data:/var/www/search/data
  #   restart: always
  scrapy:
    build:
      context: ./scrapy
      dockerfile: ./Dockerfile
    environment:
        SPLASH_URL: http://splash:8050
        IS_DOCKER: 'true'
        # PRIVATE_IP: '172.26.14.59'
        # PUBLIC_IP: '3.209.95.87'
    depends_on:
      - splash
    volumes:
      # - ./scrapy/classified:/var/www/scrapy/data/classified
      # - ./scrapy/scraped:/var/www/scrapy/data/scraped
      - ./scrapy/alegreme:/var/www/scrapy/alegreme
      - ./scrapy/data:/var/www/scrapy/data
      - scrapydo_data:/var/www/scrapy/projects
    ports:
      - "7654:7654"
    restart: always
    mem_limit: 4048000000
    command: scrapy-do --nodaemon --pidfile= scrapy-do --config scrapydo.conf
  splash:
    image: scrapinghub/splash
    ports:
      - "8050:8050"
      - "5023:5023"
    restart: always
    volumes:
      - splash_data:/var/www/splash/data
    mem_limit: 3048000000
    command: --maxrss 3048 --max-timeout 300 --disable-lua-sandbox
